{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from CSV data file\n",
    "The CSV file has been exported from Google Sheets survey responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('responses.csv', index_col=None, parse_dates=['Timestamp'])\n",
    "\n",
    "# make the timestamp into a proper datetime64 dtype\n",
    "#df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up column names\n",
    "The original column names are the questions.... let's simplifies them.  And while we're at it, set appropriate columns as categorical data to speed up analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace long column titles with shorter versions\n",
    "df.columns = [ 'date', 'neighborhood', 'demographic', 'schools', 'child_bus_freq', 'child_bike_freq', 'child_driven_freq', 'child_drive_freq', 'child_walk_freq', 'child_drive_reason', 'child_no_walk_reason', 'child_no_bike_reason', 'walk_freq', 'bike_freq', 'bikes_on_sidewalk', 'self_jog_frequency', 'commutes', 'child_self_school', 'child_self_bus_freq', 'child_self_bike_freq', 'child_self_driven_freq', 'child_self_drive_freq', 'child_self_walk_freq', 'child_self_commutes', 'child_self_has_children', 'commuter_distance', 'commuter_type', 'commuter_walk_to_station_freq', 'commuter_bike_to_station_freq', 'commuter_drive_to_station_freq', 'commuter_carpool_to_station_freq', 'commuter_driven_to_station_freq', 'commuter_bus_to_station_freq', 'no_walk_reason', 'no_bike_reason', 'drive_reason', 'feelings', 'problem_areas', 'drivers_are_safe', 'bicyclists_are_safe', 'suggested_improvements', 'additional_comments', 'owns_business', 'business_type', 'business_space', 'business_pedestrian_synergy', 'business_bicyclists_synergy', 'business_bike_rack_interest', 'business_promotion_interest', 'business_additional_comments', 'final_comments', 'contact_interest', 'contact_info', 'wants_pdf' ]\n",
    "\n",
    "# a list of the columns that contain categorical data\n",
    "categorical_columns = [ 'neighborhood', 'demographic', 'schools', 'child_bus_freq', 'child_bike_freq', 'child_driven_freq', 'child_drive_freq', 'child_walk_freq', 'child_drive_reason', 'child_no_walk_reason', 'child_no_bike_reason', 'walk_freq', 'bike_freq', 'bikes_on_sidewalk', 'self_jog_frequency', 'commutes', 'child_self_school', 'child_self_bus_freq', 'child_self_bike_freq', 'child_self_driven_freq', 'child_self_drive_freq', 'child_self_walk_freq', 'child_self_commutes', 'child_self_has_children', 'commuter_distance', 'commuter_type', 'commuter_walk_to_station_freq', 'commuter_bike_to_station_freq', 'commuter_drive_to_station_freq', 'commuter_carpool_to_station_freq', 'commuter_driven_to_station_freq', 'commuter_bus_to_station_freq', 'no_walk_reason', 'no_bike_reason', 'drive_reason', 'drivers_are_safe', 'bicyclists_are_safe', 'suggested_improvements', 'owns_business', 'business_type', 'business_space', 'business_pedestrian_synergy', 'business_bicyclists_synergy', 'business_bike_rack_interest', 'business_promotion_interest', 'contact_interest', 'wants_pdf' ]\n",
    "for col in categorical_columns :\n",
    "    df[col] = df[col].astype('category')\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove commas from multiple answer question options\n",
    "This helps analyze questions that allow for more than one answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schools': ['PVC', 'CET', 'CHHS', 'Homeschooled'],\n",
       " 'child_drive_reason': ['No - we do not drive or prefer not to drive',\n",
       "  'Our own personal preference',\n",
       "  'Lack of available busing where we live',\n",
       "  'The bus schedule does not match our schedule',\n",
       "  'Safety concerns with buses',\n",
       "  'Safety concerns with walking',\n",
       "  'Safety concerns with bicycling',\n",
       "  \"My child's health condition\"],\n",
       " 'child_no_walk_reason': ['No - they walk a lot',\n",
       "  'My child does not like to walk',\n",
       "  'We live too far to walk',\n",
       "  \"We don't have time to walk\",\n",
       "  'Fear of dangerous driving',\n",
       "  'Lack of adequate sidewalks',\n",
       "  'Lack of adequate crosswalks at busy intersections',\n",
       "  'Lack of crossing guards at busy intersections',\n",
       "  \"My child's health condition\",\n",
       "  'Visually unappealing route'],\n",
       " 'child_no_bike_reason': ['No - they bicycle a lot',\n",
       "  'My child does not like to bicycle',\n",
       "  'My child is too young to bicycle',\n",
       "  'We live too far to bicycle',\n",
       "  'Our own personal preference',\n",
       "  'Fear of dangerous driving',\n",
       "  'Lack of adequate bike lanes',\n",
       "  \"My child's health condition\",\n",
       "  'Visually unappealing route',\n",
       "  'Hills']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# a dictionary of columns that contain comma-separated multiple answer options\n",
    "multiple_answer_options = {\n",
    "    'schools': ['PVC', 'CET', 'CHHS', 'Homeschooled'],\n",
    "    'child_drive_reason': ['No - we do not drive, or prefer not to drive', 'Our own personal preference', 'Lack of available busing where we live', 'The bus schedule does not match our schedule', 'Safety concerns with buses', 'Safety concerns with walking', 'Safety concerns with bicycling', \"My child's health condition\"],\n",
    "    'child_no_walk_reason': ['No - they walk a lot', 'My child does not like to walk', 'We live too far to walk', \"We don't have time to walk\", 'Fear of dangerous driving', 'Lack of adequate sidewalks', 'Lack of adequate crosswalks at busy intersections', 'Lack of crossing guards at busy intersections', \"My child's health condition\", 'Visually unappealing route'], \n",
    "    'child_no_bike_reason': ['No - they bicycle a lot', 'My child does not like to bicycle', 'My child is too young to bicycle', 'We live too far to bicycle', 'Our own personal preference', 'Fear of dangerous driving', 'Lack of adequate bike lanes', \"My child's health condition\", 'Visually unappealing route', 'Hills']\n",
    "    # other option available for some\n",
    "}\n",
    "\n",
    "# loop through all questions that allow more than one answer\n",
    "for question, answer_list in multiple_answer_options.items():\n",
    "    # loop through every answer in the answer list for this question\n",
    "    for answer in answer_list:\n",
    "        # remove the commas, from each answer, if any\n",
    "        if ',' in answer:\n",
    "            # update the answers with a cleaned version\n",
    "            df[question] = df[question].str.replace(answer, answer.replace(',', '') )\n",
    "            \n",
    "    # remove commas from the list of answer in the dictionary\n",
    "    multiple_answer_options[question] = [answer.replace(',' , '') for answer in answer_list]\n",
    "    #display(multiple_answer_options[question])\n",
    "\n",
    "multiple_answer_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up multiple answer questions into multiple columns\n",
    "This will allow us to independently count how many respondents included each answer option in their multiple answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7a70106c4377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmultiple_answer_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# process each response from the user to this question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m',\\s*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorize_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manswer_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3594\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3595\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3596\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3597\u001b[0m             return self._constructor(mapped,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    433\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    409\u001b[0m         return _list_of_series_to_arrays(data, columns,\n\u001b[1;32m    410\u001b[0m                                          \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                                          dtype=dtype)\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_of_series_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexer_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "def categorize_answers(response, question, answer_list):\n",
    "    # convert to a series with the answer as a key\n",
    "    indices = [] # will contain the indices for the series of answers\n",
    "    values = [] # will contain the values for the series of answers\n",
    "    \n",
    "    # skip any nan or blank values\n",
    "    if str(response) != 'nan' and type(answer_list) != float and type(question) != float and type(response) != float:\n",
    "        for answer_option in response:\n",
    "            answer_option = answer_option.strip() # remove any leading/trailing whitespace\n",
    "            if answer_option in answer_list:\n",
    "                indices.append(answer_option)\n",
    "                values.append(1)\n",
    "            else:\n",
    "                indices.append('other')\n",
    "                values.append(answer_option)\n",
    "    \n",
    "        response = pd.Series(values,index=indices).fillna(0).astype(bool)\n",
    "        \n",
    "    return response\n",
    "\n",
    "# slice up answers by comma and give each its own column\n",
    "for question, answer_list in multiple_answer_options.items():\n",
    "    # process each response from the user to this question\n",
    "    df[question] = df[question].str.split(',\\s*').apply(categorize_answers, question=question, answer_list=answer_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up neighborhood names\n",
    "Neighorhood names were verbose in the actual survey... and some respondents wrote their own locations.  We group those respondents into their nearest neighborhoods and use consistent neighborhood names here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['neighborhood'] = df['neighborhood'].str.strip() # remove white space\n",
    "df['neighborhood'] = df['neighborhood'].replace({\n",
    "    \n",
    "    # simplifying neighborhood names\n",
    "    'Albany Post Road / Prickly Pear Hill / Scenic Dr area': 'Albany Post Road', \n",
    "    'Cortlandt outside of Croton': 'Cortlandt',\n",
    "    'Old Post Road N area': 'Old Post Road North',\n",
    "    'Quaker Ridge / Quaker Bridge area': 'Quaker Ridge',\n",
    "    'Sunset Park area': 'Sunset Park',\n",
    "    'Teatown area': 'Teatown',\n",
    "    'Upper Village (the area nearest the Black Cow coffee shop)': 'Upper Village',\n",
    "\n",
    "    # lumping CET/library area into Harmon\n",
    "    'CET': 'Harmon', \n",
    "    'By CET': 'Harmon',\n",
    "    'By the library/cemetery/CET': 'Harmon',\n",
    "    'End of Cleveland near path': 'Harmon',\n",
    "    'Cleveland near CET/Library': 'Harmon',\n",
    "    'Cleveland/Park': 'Harmon',\n",
    "    'Duck Pond': 'Harmon',\n",
    "    'Harmon Park': 'Harmon',\n",
    "    'Irving Ave': 'Harmon',\n",
    "    'Ridge Rd.': 'Harmon',\n",
    "    'Truesdale Drive': 'Harmon',\n",
    "    'along the croton river': 'Harmon',\n",
    "    'Behind high school': 'Harmon',\n",
    "    'Wells/Beekman Area': 'Harmon',\n",
    "    \n",
    "    # lumping Old Post Road South and Sunset Park together\n",
    "    'Old Post Road S': 'Sunset Park', \n",
    "    \n",
    "    # lumping North Riverside area into Croton Landing\n",
    "    'North Riverside': 'Croton Landing',\n",
    "    'Palmer on High St': 'Croton Landing',\n",
    "    'lower village': 'Croton Landing',\n",
    "    'Brook St': 'Croton Landing',\n",
    "    \n",
    "    # lumping Old Post Road North and Croton Landing together\n",
    "    'Croton Landing': 'Old Post Road North', \n",
    "    'wolf road': 'Albany Post Road',\n",
    "    \n",
    "    # lumping nearby streets into Upper Village\n",
    "    'Bari Manor': 'Upper Village',\n",
    "    'Harrison st': 'Upper Village',\n",
    "    'Wells Ave': 'Upper Village',\n",
    "    \n",
    "    # lumping Batten Rd and Crompond Rd areas with Mount Airy\n",
    "    'Batten Road': 'Mount Airy',\n",
    "    'The trails': 'Mount Airy',\n",
    "    '129 near dam': 'Mount Airy', \n",
    "    \n",
    "    #lumping Teatown into Quaker Ridge, since there were few Teatown respondents\n",
    "    'Teatown': 'Quaker Ridge' \n",
    "})\n",
    "\n",
    "df['neighborhood'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up demographics\n",
    "Some respondents indicated their own demographic titles... we're standardizing these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['demographic'] = df['demographic'].str.strip() # remove white space\n",
    "df['demographic'] = df['demographic'].replace({\n",
    "    \n",
    "    # creating new category for adults with small children\n",
    "    'Adult with a 22 year old and a 3 year old living with me': 'Adult with small child',\n",
    "    'Adult with an infant': 'Adult with small child',\n",
    "    'Adult with new baby': 'Adult with small child',\n",
    "    'Adult with toddler': 'Adult with small child',\n",
    "    'Adult with toddler living with me': 'Adult with small child',\n",
    "    'Adult with toddlers living with me': 'Adult with small child',\n",
    "    'Adult with young children': 'Adult with small child',\n",
    "    'Adult with young children living with me': 'Adult with small child',\n",
    "    'Adult with 3year old': 'Adult with small child',\n",
    "    'Adult with children not yet in CET (pre-K)': 'Adult with small child',\n",
    "    \n",
    "    # lump adults with infants & toddlers into adults with no school age children\n",
    "    'Adult with small child': 'Adult with no school age children living with me',\n",
    "    'Adult with spouse and adult children living with us.': 'Adult with no school age children living with me',\n",
    "    \n",
    "    # lumping adults with college kids into the adults with no school age children category\n",
    "    'adult with post college child living with me': 'Adult with no school age children living with me',\n",
    "    'Adult with College Children': 'Adult with no school age children living with me',\n",
    "    'Adult with College Children': 'Adult with no school age children living with me',\n",
    "\n",
    "        # lump adults with school children in addition to others\n",
    "    'Adult with school age children and a senior living with me.': 'Adult with school kid'\n",
    "\n",
    "})    \n",
    "\n",
    "# simplify response text\n",
    "df['demographic'] = df['demographic'].replace({\n",
    "    'Adult with school age children living with me': 'Adult with school kid',\n",
    "    'Adult with no school age children living with me': 'Adult without school kid',\n",
    "    'College student': 'College kid',\n",
    "    'High School student': 'High School kid',\n",
    "    'Middle School student': 'Middle School kid',\n",
    "})\n",
    "\n",
    "df['demographic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up columns with comma-separated values into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dataframe with each school in its own column, and 0 or 1 as the values indicating which row indicated that school\n",
    "#df2 = df[pd.notnull(df['schools'])]\n",
    "#df3 = df2['schools'].str.get_dummies(sep=', ')\n",
    "#for column in df3:\n",
    "#    column = column.strip() #remove whitespace\n",
    "#df3\n",
    "\n",
    "# split a column with comma-separated values into separate columns\n",
    "def breakout_comma_separated_values(old_column_name):\n",
    "    # expand comma-separated values in the column into their own dataframe with multiple columns for each value\n",
    "    df[old_column_name] = df[old_column_name].str.split(', ') #split by comma into a list\n",
    "    series = df[old_column_name].apply(pd.Series) # convert each list into a series\n",
    "\n",
    "    # rename each variable with a prefixed column name in a new dataframe\n",
    "    new_df = series.rename(columns = lambda x : old_column_name + '_' + str(x))\n",
    "    \n",
    "    # return the new dataframe\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# try it out\n",
    "fields = [ 'schools', 'child_drive_reason', 'child_no_walk_reason', 'child_no_bike_reason', 'no_walk_reason', 'no_bike_reason', 'drive_reason' ]\n",
    "#fields = ['schools']\n",
    "\n",
    "for column_name in fields:\n",
    "    \n",
    "    # split up the column into multiple columns\n",
    "    df2 = breakout_comma_separated_values(column_name)\n",
    "    \n",
    "    # join the new dataframe to the original dataframe\n",
    "    df = pd.concat([df[:], df2[:]], axis=1)\n",
    "\n",
    "    # delete the original column from the original dataframe\n",
    "    del df[column_name]\n",
    "\n",
    "    display(df2)\n",
    "    \n",
    "column_names = [c for c in df]\n",
    "display(column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned up data to CSV file\n",
    "So it can be analyzed in subsequent programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('responses_scrubbed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
